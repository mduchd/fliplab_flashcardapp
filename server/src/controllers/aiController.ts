import { Request, Response } from 'express';
import { GoogleGenerativeAI } from '@google/generative-ai';

// --- HELPERS ---

const getGenerativeModel = () => {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) return null;
    const genAI = new GoogleGenerativeAI(apiKey);
    return genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
};

// Fallback logic for Chat
const getMockChatResponse = (message: string): string => {
    const lower = message.toLowerCase();
    if (lower.includes('ch√†o') || lower.includes('hello')) return "Ch√†o b·∫°n! M√¨nh l√† FlipLab AI. Ch√∫c b·∫°n m·ªôt ng√†y h·ªçc t·∫≠p nƒÉng su·∫•t! üöÄ";
    if (lower.includes('t·∫°o th·∫ª')) return "B·∫°n c√≥ th·ªÉ d√πng n√∫t 'Magic AI' trong trang 'T·∫°o b·ªô th·∫ª' ƒë·ªÉ m√¨nh gi√∫p b·∫°n so·∫°n b√†i nhanh nh√©! ‚ú®";
    if (lower.includes('c·∫£m ∆°n')) return "Kh√¥ng c√≥ chi! C·ªë g·∫Øng l√™n nh√©! üí™";
    if (lower.includes('bu·ªìn') || lower.includes('n·∫£n')) return "ƒê·ª´ng n·∫£n ch√≠! 'H·ªçc t·∫≠p l√† h·∫°t gi·ªëng c·ªßa ki·∫øn th·ª©c, ki·∫øn th·ª©c l√† h·∫°t gi·ªëng c·ªßa h·∫°nh ph√∫c'. Ngh·ªâ ng∆°i ch√∫t r·ªìi ti·∫øp t·ª•c n√†o! ‚òï";
    return "Th√∫ v·ªã ƒë√≥! Nh∆∞ng hi·ªán t·∫°i k·∫øt n·ªëi c·ªßa m√¨nh h∆°i ch·∫≠p ch·ªùn, b·∫°n h·ªèi l·∫°i sau nh√© ho·∫∑c th·ª≠ h·ªèi v·ªÅ c√°ch h·ªçc xem?";
};

// Fallback logic for Flashcard Generation
const getMockData = (topic: string, count: number) => {
    const mocks = [
        { term: "Artificial Intelligence", definition: "Tr√≠ tu·ªá nh√¢n t·∫°o (AI) - M√¥ ph·ªèng tr√≠ tu·ªá con ng∆∞·ªùi." },
        { term: "Machine Learning", definition: "H·ªçc m√°y - M·ªôt nh√°nh c·ªßa AI gi√∫p m√°y t√≠nh t·ª± h·ªçc t·ª´ d·ªØ li·ªáu." },
        { term: "Deep Learning", definition: "H·ªçc s√¢u - M·∫°ng n∆°-ron nh√¢n t·∫°o nhi·ªÅu l·ªõp." },
        { term: "Neural Network", definition: "M·∫°ng n∆°-ron - H·ªá th·ªëng m√¥ ph·ªèng n√£o b·ªô sinh h·ªçc." },
        { term: "Algorithm", definition: "Thu·∫≠t to√°n - T·∫≠p h·ª£p c√°c quy t·∫Øc t√≠nh to√°n." }
    ];
    return mocks.slice(0, count);
};

// --- CACHE & RATE LIMIT STORE (In-Memory) ---
const responseCache = new Map<string, { data: any, timestamp: number }>();
const userRateLimit = new Map<string, { count: number, resetAt: number }>();
const userDailyUsage = new Map<string, { count: number, date: string }>(); // New: Daily Tracker

const CACHE_TTL = 60 * 60 * 1000; // 1 Hour
const RATE_LIMIT_WINDOW = 60 * 1000; // 1 Minute
const MAX_RPM = 10; // Spam protection (10 req/min)
const MAX_RPD = 20; // Hard Limit (20 req/day - Gemini 2.5 Free Tier)

// Helper: Check Daily Quota
const checkDailyQuota = (userId: string): { allowed: boolean, remaining: number, used: number } => {
    const today = new Date().toISOString().split('T')[0]; // YYYY-MM-DD
    const record = userDailyUsage.get(userId) || { count: 0, date: today };

    // Reset if new day
    if (record.date !== today) {
        record.date = today;
        record.count = 0;
    }

    if (record.count >= MAX_RPD) {
        return { allowed: false, remaining: 0, used: record.count };
    }

    return { allowed: true, remaining: MAX_RPD - record.count, used: record.count };
};

// Helper: Increment Usage
const incrementUsage = (userId: string) => {
    const today = new Date().toISOString().split('T')[0];
    const record = userDailyUsage.get(userId) || { count: 0, date: today };
    if (record.date !== today) { record.count = 0; record.date = today; } // Double check
    record.count++;
    userDailyUsage.set(userId, record);
    return MAX_RPD - record.count;
};

// Helper: Check Rate Limit (Spam Protection)
const checkRateLimit = (userId: string): boolean => {
    const now = Date.now();
    const userRecord = userRateLimit.get(userId) || { count: 0, resetAt: now + RATE_LIMIT_WINDOW };

    if (now > userRecord.resetAt) {
        userRecord.count = 1;
        userRecord.resetAt = now + RATE_LIMIT_WINDOW;
    } else {
        userRecord.count++;
    }
    userRateLimit.set(userId, userRecord);
    return userRecord.count <= MAX_RPM;
};


// Helper: Get/Set Cache
const getCachedResponse = (key: string) => {
    const cached = responseCache.get(key);
    if (!cached) return null;
    if (Date.now() - cached.timestamp > CACHE_TTL) {
        responseCache.delete(key);
        return null; // Expired
    }
    return cached.data;
};

const setCachedResponse = (key: string, data: any) => {
    // Basic memory management: Clear older entries if too big
    if (responseCache.size > 1000) responseCache.clear(); 
    responseCache.set(key, { data, timestamp: Date.now() });
};

// --- CONTROLLERS ---

// 1. Generate Flashcards
export const generateFlashcards = async (req: Request, res: Response) => {
    // @ts-ignore
    const userId = req.user?.id || 'anonymous';
    
    // 1. Daily Quota Check
    const quota = checkDailyQuota(userId);
    if (!quota.allowed) {
        return res.status(429).json({ 
            message: 'ƒê√£ h·∫øt l∆∞·ª£t d√πng AI h√¥m nay (20/20). Vui l√≤ng quay l·∫°i ng√†y mai!' 
        });
    }

    // 2. Rate Limit Check (Spam)
    if (!checkRateLimit(userId)) {
        return res.status(429).json({ message: 'Thao t√°c qu√° nhanh! Vui l√≤ng ƒë·ª£i gi√¢y l√°t.' });
    }

    const { prompt, count = 10, topic } = req.body;
    if (!prompt && !topic) return res.status(400).json({ message: 'Vui l√≤ng cung c·∫•p ch·ªß ƒë·ªÅ' });

    // 3. Cache Check
    const cacheKey = `gen_card_${topic || prompt}_${count}`;
    const cached = getCachedResponse(cacheKey);
    if (cached) {
        console.log('‚ö° Using Cached AI Response');
        return res.json({ 
            suggestions: cached,
            usage: { used: quota.used, total: MAX_RPD, remaining: quota.remaining } // Cache doesn't consume quota
        });
    }

    try {
        const model = getGenerativeModel();
        if (!model) throw new Error('No API Key');

        console.log('ü§ñ Generative AI: Generating cards...');
        
        const userContent = topic 
            ? `T·∫°o ${count} thu·∫≠t ng·ªØ flashcard v·ªÅ ch·ªß ƒë·ªÅ: "${topic}"`
            : `Tr√≠ch xu·∫•t ${count} thu·∫≠t ng·ªØ quan tr·ªçng t·ª´ vƒÉn b·∫£n: "${prompt}"`;

        const finalPrompt = `
            B·∫°n l√† tr·ª£ l√Ω gi√°o d·ª•c. Nhi·ªám v·ª•: Tr·∫£ v·ªÅ JSON Array thu·∫ßn t√∫y (kh√¥ng markdown).
            M·ªói object: {"term": "...", "definition": "..."} (Gi·∫£i th√≠ch ti·∫øng Vi·ªát ng·∫Øn g·ªçn).
            Input: ${userContent}
        `;

        const result = await model.generateContent(finalPrompt);
        const response = await result.response;
        let text = response.text().replace(/```json/g, '').replace(/```/g, '').trim();
        const start = text.indexOf('[');
        const end = text.lastIndexOf(']');
        if (start !== -1 && end !== -1) text = text.substring(start, end + 1);

        const flashcards = JSON.parse(text);
        if (Array.isArray(flashcards)) {
            // 4. Save to Cache & Increment Usage
            setCachedResponse(cacheKey, flashcards);
            const remaining = incrementUsage(userId); // Consume Quota
            
            return res.json({ 
                suggestions: flashcards,
                usage: { used: quota.used + 1, total: MAX_RPD, remaining }
            });
        }
        throw new Error('Invalid Format');

    } catch (error: any) {
        console.error('‚ùå AI Generate Error (Fallback mock):', error.message);
        await new Promise(r => setTimeout(r, 1500));
        return res.json({ 
            suggestions: getMockData(topic || prompt, count),
            isMock: true,
            message: "H·ªá th·ªëng AI b·∫≠n, d√πng d·ªØ li·ªáu m·∫´u.",
            usage: { used: quota.used, total: MAX_RPD, remaining: quota.remaining }
        });
    }
};

// 2. Chat Assistant
export const chatWithAI = async (req: Request, res: Response) => {
    const { message, style = 'friendly' } = req.body;
    // @ts-ignore
    const userId = req.user?.id || 'anonymous'; 

    if (!message) return res.status(400).json({ reply: 'B·∫°n ch∆∞a n√≥i g√¨ c·∫£...' });

    // 1. Daily Quota Check
    const quota = checkDailyQuota(userId);
    if (!quota.allowed) {
        return res.json({ 
            reply: 'üíî B·∫°n ƒë√£ d√πng h·∫øt 20 l∆∞·ª£t AI mi·ªÖn ph√≠ h√¥m nay. Quay l·∫°i v√†o ng√†y mai nh√© ho·∫∑c d√πng t√≠nh nƒÉng T·∫°o th·∫ª!',
            isOverQuota: true
        });
    }

    // 2. Rate Limit Check
    if (!checkRateLimit(userId)) {
        return res.json({ reply: '‚è≥ B·∫°n h·ªèi nhanh qu√°! Cho m√¨nh ngh·ªâ tay x√≠u nh√© (Rate limit).' });
    }

    // 3. Cache Check
    const cacheKey = `chat_${style}_${message.toLowerCase().trim()}`;
    const cached = getCachedResponse(cacheKey);
    if (cached) {
        console.log('‚ö° Using Cached Chat Response');
        return res.json({ 
            reply: cached,
            usage: { used: quota.used, total: MAX_RPD, remaining: quota.remaining }
        });
    }

    try {
        const model = getGenerativeModel();
        if (!model) throw new Error('No API Key');
        
        console.log(`ü§ñ Generative AI: Chatting with user ${userId} [Style: ${style}]...`);

        // Define Personas
        let personaPrompt = '';
        switch (style) {
            case 'professional':
                personaPrompt = 'Persona: B·∫°n l√† Gi√°o s∆∞ Ng√¥n ng·ªØ h·ªçc uy√™n b√°c. Tr·∫£ l·ªùi ch√≠nh x√°c, trang tr·ªçng, d√πng t·ª´ v·ª±ng n√¢ng cao, kh√¥ng d√πng emoji c·ª£t nh·∫£.';
                break;
            case 'concise':
                personaPrompt = 'Persona: B·∫°n l√† Tr·ª£ l√Ω AI t·ªëi gi·∫£n. Tr·∫£ l·ªùi c·ª±c ng·∫Øn g·ªçn (d∆∞·ªõi 30 t·ª´), ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, kh√¥ng r∆∞·ªùm r√† kh√°ch s√°o.';
                break;
            case 'socratic':
                personaPrompt = 'Persona: B·∫°n l√† Nh√† tri·∫øt h·ªçc Socratic. KH√îNG tr·∫£ l·ªùi ngay. H√£y ƒë·∫∑t c√¢u h·ªèi g·ª£i m·ªü ƒë·ªÉ ng∆∞·ªùi d√πng t·ª± t√¨m ra c√¢u tr·∫£ l·ªùi. Ch·ªâ gi·∫£i th√≠ch khi ng∆∞·ªùi d√πng th·ª±c s·ª± b√≠.';
                break;
            default: // friendly
                personaPrompt = 'Persona: B·∫°n l√† FlipLab AI - tr·ª£ l√Ω h·ªçc t·∫≠p c·ª±c k·ª≥ nhi·ªát t√¨nh, h√†i h∆∞·ªõc v√† th√¢n thi·ªán. D√πng nhi·ªÅu emoji ƒë·ªÉ c·ªï v≈© tinh th·∫ßn.';
                break;
        }

        const prompt = `
            Context: B·∫°n ƒëang chat v·ªõi User ID: ${userId}.
            ${personaPrompt}
            Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa User theo ƒë√∫ng Persona tr√™n.
            User Question: "${message}"
            AI Reply:
        `;

        const result = await model.generateContent(prompt);
        const response = await result.response;
        const reply = response.text();

        // 4. Save to Cache & Increment Usage
        setCachedResponse(cacheKey, reply);
        const remaining = incrementUsage(userId);

        return res.json({ 
            reply, 
            usage: { used: quota.used + 1, total: MAX_RPD, remaining }
        });

    } catch (error: any) {
        console.error('‚ùå AI Chat Error (Fallback mock):', error.message);
        await new Promise(r => setTimeout(r, 1000));
        return res.json({ reply: getMockChatResponse(message) });
    }
};
